{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasons for negative comments about airlines\n",
    "\n",
    "The exercise is concerned with a dataset of tweets that mention several US airline companies. Each tweet was manually labelled as expressing negative sentiment. In addition, each tweet was marked up for the reason for the negative sentiment, with labels such as \"Customer Service Issue\", \"Late Flight\", \"Lost Luggage\".\n",
    "\n",
    "The task is to train and evaluate an SVM classifier of the reasons for negative sentiment. A part of the solution is provided below, you need to supply your code in places indicated with \"???\"\n",
    "\n",
    "In the training stage, you should try several values for `C`. Find an optimal setting for it.\n",
    "\n",
    "Provide comments on the learning curve and confusion matrix plots. Suggest ways to improve the accuracy of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T17:58:36.733396Z",
     "start_time": "2023-06-08T17:58:34.303940Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "Load the dataset from the URL, select only the \"text\" and \"negativereason\" columns, and drop any rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-08T17:58:39.709471Z",
     "start_time": "2023-06-08T17:58:38.376863Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/chawit/data-Airline/master/TweetsAirline.csv\",\n",
    "                usecols=[\"text\", \"negativereason\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with NaN\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"negativereason\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning and transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into the training and test parts\n",
    "# the test part should be 0.2 of the whole dataset\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(docs_train)} train and {len(docs_test)} test instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def custom_preprocessor(text):\n",
    "    \"\"\"Remove some elements of a tweet (URLs, hashtags, Twitter handles,\n",
    "    any other non-words) using regular expressions\n",
    "    \"\"\"\n",
    "    # RE for a URL\n",
    "    urls = re.compile(r'https?:\\/\\/(www\\.)?[a-z]+\\.(com|net|org|ly)\\S+')\n",
    "    \n",
    "    # RE for a hashtag, e.g. #MachineLearning #DeepLearning\n",
    "    hashtags = ???\n",
    "    \n",
    "    # RE for a Twitter handle, e.g., @scikit_learn, @ogrisel\n",
    "    handles = ???\n",
    "    \n",
    "    text = urls.sub('', text)\n",
    "    text = hashtags.sub('', text)\n",
    "    text = handles.sub('', text)\n",
    "    \n",
    "    # bonus question: remove any sequences of two or more characters that are not\n",
    "    # letters, digits or spaces, e.g. \":)\",  \"!!!\", \"<<<\", etc.\n",
    "    nonwords = re.compile(???)\n",
    "    text = nonwords.sub('', text)\n",
    "    \n",
    "    # lower-case everything\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    strip_accents=\"unicode\", # convert accented chars to non-accented versions\n",
    "    lowercase=True,\n",
    "    tokenizer=None,        # None - use the default tokenizer\n",
    "    preprocessor=custom_preprocessor,     # None - use the default preprocessor\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1,1),     # min and max range of ngrams\n",
    "    analyzer=\"word\",       # split the document into words, rather than e.g. characters\n",
    "    max_df=1.0,            # ignore words with df greater than the value (int represents count, \n",
    "                           # float represents proportion of documents)\n",
    "    min_df=0.007           # ignore words the df lower than the value (int represents count, \n",
    "                           # float represents proportion)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a document-by-word matrix for training data\n",
    "docs_train_counts = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a document-by-word matrix for test data\n",
    "docs_test_counts = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform word counts in each document to TFIDF weights\n",
    "\n",
    "???\n",
    "\n",
    "# fit and transform the training set with \"fit_transform()\"\n",
    "docs_train_tfidf = ???\n",
    "\n",
    "# transform test\n",
    "docs_test_tfidf = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data with MaxAbsScaler\n",
    "\n",
    "X_train = ???\n",
    "X_test = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline\n",
    "\n",
    "Calculate the F-score for the majority baseline (every label is \"Customer Service Issue\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, counts = np.unique(y_train, return_counts=True)\n",
    "total = counts.sum()\n",
    "fscores = []\n",
    "for k, v in zip(labels, counts):\n",
    "    if k == \"Customer Service Issue\":\n",
    "        p = v / total\n",
    "        r = 1.0\n",
    "        f = 2/(1/p + 1/r)\n",
    "        fscores.append(f)\n",
    "        print(f\"{k:30} {v:5} fscore: {f:.3}\")\n",
    "    else:\n",
    "        fscores.append(0)\n",
    "        print(f\"{k:30} {v:5} fscore: 0.0\")\n",
    "\n",
    "macroaveraged_f = ???\n",
    "print(f\"\\nMacroaveraged baseline: {macroaveraged_f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lsvm = LinearSVC(random_state=7, max_iter=10000)\n",
    "\n",
    "# specify the hyperparameters and their values\n",
    "param_grid = {\n",
    "    'C': [???]\n",
    "}\n",
    "\n",
    "# we'll use 5-fold cross-validation\n",
    "grid_search = GridSearchCV(lsvm, param_grid, cv=5,\n",
    "                           scoring='f1_macro', \n",
    "                           return_train_score=True) \n",
    "\n",
    "start = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "end = time.time() - start\n",
    "print(f\"Took {end} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(grid_search.cv_results_)[['params', 'mean_train_score', 'mean_test_score']]\n",
    "cv_results[\"diff, %\"] = 100*(cv_results[\"mean_train_score\"]-cv_results[\"mean_test_score\"]\n",
    "                                                     )/cv_results[\"mean_train_score\"]\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "cv_results.sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def plot_learning_curves(model, X, y):\n",
    "    \n",
    "    # create a train-test split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=7)\n",
    "    train_errors, val_errors, sizes = [], [], []\n",
    "    \n",
    "    # take different sizes of the training data, starting from 50s instance, with the step of 10\n",
    "    for m in range(50, X_train.shape[0], 100):\n",
    "        \n",
    "        # fit a model\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        \n",
    "        # evaluate the model on both training and validation sets\n",
    "        yhat_train = model.predict(X_train[:m])\n",
    "        yhat_val = model.predict(X_val)\n",
    "        \n",
    "        # calculate F-score and record the results\n",
    "        p, r, train_fscore, s = precision_recall_fscore_support(yhat_train,\n",
    "                                                                y_train[:m], average=\"macro\")\n",
    "        p, r, val_fscore, s = precision_recall_fscore_support(yhat_val, y_val, average=\"macro\")\n",
    "                \n",
    "        train_errors.append(train_fscore)\n",
    "        val_errors.append(val_fscore)\n",
    "        sizes.append(m)\n",
    "        \n",
    "    # make a plot\n",
    "    temp_df = pd.DataFrame({\"training set size\": sizes, \n",
    "                        \"train\": train_errors,\n",
    "                        \"val\": val_errors})\n",
    "\n",
    "    temp_df.plot(x=\"training set size\", y=[\"train\", \"val\"], kind=\"line\", \n",
    "             figsize=(6, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvm = LinearSVC(???, random_state=7, max_iter=10000)\n",
    "\n",
    "plot_learning_curves(lsvm, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-train the best model found during cross-validation\n",
    "\n",
    "lsvm = LinearSVC(???, random_state=7, max_iter=10000).fit(X_train, y_train)\n",
    "yhat = lsvm.predict(X_test)\n",
    "\n",
    "# micro-averaged precision, recall and f-score\n",
    "p, r, f, s = precision_recall_fscore_support(y_test, yhat, average=\"macro\")\n",
    "print(f\"Precision: {p}\")\n",
    "print(f\"Recall: {r}\")\n",
    "print(f\"F score: {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (12, 12)\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(lsvm, X_test, y_test,\n",
    "                                 display_labels=lsvm.classes_,\n",
    "                                 xticks_rotation=45,\n",
    "                                 cmap='Blues',\n",
    "                                 normalize='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
