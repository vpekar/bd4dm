{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be857f0e",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Customer Reviews: A Small Dataset\n",
    "\n",
    "The exercise concerns classification of mobile phone reviews into positive, negative and neutral. The data contains 2k reviews and is characterised by class imbalance: there are much more positive reviews than negative ones in the dataset. Your task is to train two Linear SVM classifiers: one on unbalanced data and one after applying a class-balancing technique. Decide on the technique to use and find out if it helps to improve classification accuracy on the test data.\n",
    "\n",
    "Note that rather than Macroaveraged F score the project uses Macroaveraged RMSE ([Baccianella et al 2009](http://nmis.isti.cnr.it/sebastiani/Publications/ISDA09.pdf)) as an evaluation metric.\n",
    "\n",
    "Propose a solution by writing code and commentary instead of \"???\" in the cells below.\n",
    "\n",
    "The data can be obtained at this [page](https://jmcauley.ucsd.edu/data/amazon/), download the file [reviews_Cell_Phones_and_Accessories_5.json.gz](http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Cell_Phones_and_Accessories_5.json.gz) and put it into the same folder as the notebook.\n",
    "\n",
    "To run the code, you may need to first install the imblearn package:\n",
    "\n",
    "`pip install -U imbalanced-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d1017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"SKLEARNEX\").setLevel(logging.ERROR)\n",
    "\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6e2e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupby_labels(y, yhat):\n",
    "    \"\"\"Based on https://stackoverflow.com/questions/38013778/is-there-any-numpy-group-by-function\n",
    "    \"\"\"\n",
    "    m = np.stack([y, yhat]).T\n",
    "    m = m[m[:, 0].argsort()]\n",
    "    grouped_preds = np.split(m[:, 1], np.unique(m[:, 0], return_index=True)[1])[1:]\n",
    "    labels = np.unique(m[:, 0])\n",
    "    return labels, grouped_preds\n",
    "\n",
    "def mae_macro(y, yhat):\n",
    "    \"\"\"Macroaveraged MAE\n",
    "    \"\"\"\n",
    "    labels, preds = groupby_labels(y, yhat)\n",
    "    mean_diff = np.array([np.abs(label - pred).mean() for label, pred in zip(labels, preds)]).mean()\n",
    "    return mean_diff\n",
    "\n",
    "def rmse_macro(y, yhat):\n",
    "    \"\"\"Macroaveraged RMSE\n",
    "    \"\"\"\n",
    "    labels, preds = groupby_labels(y, yhat)\n",
    "    mean_diff = np.array([np.power(label - pred, 2).mean() for label, pred in zip(labels, preds)]).mean()\n",
    "    return np.sqrt(mean_diff)\n",
    "\n",
    "def evaluate_model(model, ytest, Xtest):\n",
    "    \"\"\"Given a trained model and test data, generate predictions\n",
    "    and print a report with evaluation results\n",
    "    \"\"\"\n",
    "    yhat = model.predict(Xtest)\n",
    "    print(classification_report(ytest, yhat, zero_division=0))\n",
    "    rmse = rmse_macro(ytest, yhat)\n",
    "    print(f\"{'Macro RMSE':18} {rmse:.3}\")\n",
    "    mae = mae_macro(ytest, yhat)\n",
    "    print(f\"{'Macro MAE':18} {mae:.3}\")\n",
    "\n",
    "def print_cv_results(grid_search, col_width=100, max_rows=10):\n",
    "    \"\"\"Given a grid search object, print a table with the \n",
    "    cross-validation results\n",
    "    \"\"\"\n",
    "    results = pd.DataFrame(grid_search.cv_results_\n",
    "                             )[['params', 'mean_train_score', 'mean_test_score']]\n",
    "    \n",
    "    results[\"mean_train_score\"] = -results[\"mean_train_score\"]\n",
    "    results[\"mean_test_score\"] = -results[\"mean_test_score\"]\n",
    "    \n",
    "    results[\"diff, %\"] = 100*(results[\"mean_train_score\"]-results[\"mean_test_score\"]\n",
    "                                                         )/results[\"mean_train_score\"]\n",
    "\n",
    "    pd.set_option('display.max_colwidth', col_width)\n",
    "    pd.set_option('display.min_rows', max_rows)\n",
    "    pd.set_option('display.max_rows', max_rows)\n",
    "    display(results.sort_values('mean_test_score', ascending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584cb285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scoring function\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "neg_rmse_macro = make_scorer(rmse_macro, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522d545a",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "Each review is provided with with a 5 star rating. Use the first 2k reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6193d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "targets = []\n",
    "max_lines = 2000\n",
    "lines = 0\n",
    "\n",
    "for line in gzip.open(\"reviews_Cell_Phones_and_Accessories_5.json.gz\", 'r'):\n",
    "    d = json.loads(line)\n",
    "    score = int(d['overall'])\n",
    "    text = d['reviewText']\n",
    "    texts.append(text)\n",
    "    targets.append(score)\n",
    "    \n",
    "    # read the first `max_lines` reviews\n",
    "    lines += 1\n",
    "    if lines >= max_lines:\n",
    "        break\n",
    "\n",
    "df = pd.DataFrame({\"text\": texts, \"target\": targets})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17ee6da",
   "metadata": {},
   "source": [
    "# Training-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008f70b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainset, testset = train_test_split(df, test_size=0.1, stratify=df[\"target\"], random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51b1391",
   "metadata": {},
   "source": [
    "# Data exploration and transformation\n",
    "\n",
    "Check the distribution of the class label in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8712b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset[\"target\"].value_counts().plot(kind=\"bar\", rot=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280f1a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = trainset.drop(\"target\", axis=1)\n",
    "ytrain = trainset[\"target\"].copy()\n",
    "\n",
    "Xtest = testset.drop(\"target\", axis=1)\n",
    "ytest = testset[\"target\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e81337d",
   "metadata": {},
   "source": [
    "## Construct document-by-word matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(\n",
    "    strip_accents=\"unicode\", # convert accented chars to non-accented versions\n",
    "    lowercase=True,\n",
    "    tokenizer=None,        # None - use the default tokenizer\n",
    "    preprocessor=None,     # None - use the default preprocessor\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1, 3),    # min and max range of ngrams\n",
    "    analyzer=\"word\",       # split the document into words, rather than e.g. characters\n",
    "    max_df=1.0,              # ignore words with df greater than the value (int represents count, \n",
    "                           # float represents proportion of documents)\n",
    "    min_df=3               # ignore words with df lower than the value (int represents count, \n",
    "                           # float represents proportion)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835de1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train_counts = count_vectorizer.fit_transform(Xtrain['text'])\n",
    "docs_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc3058",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_test_counts = count_vectorizer.transform(Xtest['text'])\n",
    "docs_test_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5f49cd",
   "metadata": {},
   "source": [
    "## TFIDF weighting of document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfa350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "docs_train_tfidf = tfidf_transformer.fit_transform(docs_train_counts)\n",
    "docs_test_tfidf = tfidf_transformer.transform(docs_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d83c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler \n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "Xtrain = scaler.fit_transform(docs_train_tfidf)\n",
    "Xtest = scaler.transform(docs_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cc96c7",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb8f8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84053bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(Xtrain, ytrain)\n",
    "yhat_train = dummy_clf.predict(Xtrain)\n",
    "\n",
    "evaluate_model(dummy_clf, ytrain, Xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece18a9",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "## Unbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90f15df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lsvm = LinearSVC(random_state=7, max_iter=10000)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10] \n",
    "}\n",
    "\n",
    "lsvc_grid_search = GridSearchCV(lsvm, param_grid, cv=10,\n",
    "                           scoring=neg_rmse_macro,\n",
    "                           return_train_score=True) \n",
    "lsvc_grid_search.fit(Xtrain, ytrain)\n",
    "\n",
    "print_cv_results(lsvc_grid_search, col_width=100, max_rows=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f836ec59",
   "metadata": {},
   "source": [
    "<mark>Comment:</mark>: ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce29201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation confusion matrix, training data\n",
    "yhat = cross_val_predict(lsvc_grid_search.best_estimator_, Xtrain, ytrain, cv=10)\n",
    "ConfusionMatrixDisplay.from_predictions(ytrain, yhat, \n",
    "                                        labels=lsvc_grid_search.best_estimator_.classes_, \n",
    "                                        normalize=\"true\",\n",
    "                                        cmap=plt.cm.Blues);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc3319d",
   "metadata": {},
   "source": [
    "<mark>Comment:</mark>: ???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf15f66a",
   "metadata": {},
   "source": [
    "## Class balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c476b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn ???\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ???\n",
    "        ('lsvc', LinearSVC(random_state=7, max_iter=10000))\n",
    "    ])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        ???\n",
    "        'lsvc__C': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
    "    },\n",
    "]\n",
    "\n",
    "cb_grid_search = GridSearchCV(pipeline, param_grid, cv=10, \n",
    "                              scoring=neg_rmse_macro,\n",
    "                              return_train_score=True)\n",
    "\n",
    "cb_grid_search.fit(Xtrain, ytrain)\n",
    "\n",
    "print_cv_results(cb_grid_search, col_width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cde078",
   "metadata": {},
   "source": [
    "<mark>Comment:</mark>: ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f994d50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cross-validation confusion matrix on the training data\n",
    "yhat = cross_val_predict(cb_grid_search.best_estimator_, Xtrain, ytrain, cv=10)\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(ytrain, yhat, \n",
    "                                        labels=cb_grid_search.best_estimator_.classes_, \n",
    "                                        normalize=\"true\",\n",
    "                                        cmap=plt.cm.Blues);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6184da5f",
   "metadata": {},
   "source": [
    "<mark>Comment:</mark>: ???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c871e35",
   "metadata": {},
   "source": [
    "# Evaluate on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491fc2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unbalanced data\n",
    "evaluate_model(lsvc_grid_search.best_estimator_, ytest, Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee20ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class balancing\n",
    "evaluate_model(cb_grid_search.best_estimator_, ytest, Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2161c257",
   "metadata": {},
   "source": [
    "<mark>Comment:</mark>: ???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b493eca",
   "metadata": {},
   "source": [
    "# Citing this notebook\n",
    "\n",
    "If you use this notebook in your work, please cite it as follows:\n",
    "    \n",
    "Pekar, V. (2023). Big Data for Decision Making. Lecture examples and exercises. (Version 1.0.0). URL: https://github.com/vpekar/bd4dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef208c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "232.727px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
