{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "codehighlighter": [
     [
      0,
      0
     ]
    ]
   },
   "source": [
    "# Forecasting time series\n",
    "\n",
    "The task is to build a Decision Tree and a Random Forest models to predict the sales amounts of furniture in a furniture shop, given past observations. The accuracy of the models should be measured in terms of RMSE and compared to a persistence baseline.\n",
    "\n",
    "The source of the data is [here](https://www.kaggle.com/pruthvi1995/superstore-sales).\n",
    "\n",
    "You need to complete the code sections and provide comments in places indicated with \"???\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting logging to print only error messages from Sklearnex\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"SKLEARNEX\").setLevel(logging.ERROR)\n",
    "\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "# import a decision tree regressor class\n",
    "???\n",
    "\n",
    "# import a random forest regressor class\n",
    "???\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Load data\n",
    "\n",
    "We will select only sales relating to Furniture, and will use only the columns \"Order Date\" and \"Sales\".\n",
    "\n",
    "Note `read_excel` will guess that \"Order Date\" contains dates and will convert the column to the datetime type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"superstore.xlsx\", usecols=[\"Order Date\", \"Sales\", \"Category\"])\n",
    "df = df.loc[df['Category'] == 'Furniture']\n",
    "\n",
    "# once the relevant rows have been selected, delete the Category column\n",
    "del df[\"Category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain daily amounts of sales\n",
    "df = df.groupby('Order Date').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(figsize=(16,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The daily values appear to be quite volatile, so we will group the data into weeks and recording the total of sales for that week.\n",
    "\n",
    "This can be achieved with the `resample` method of the dataframe. Before the method can be used, the dataframe must be set to have a datetime index. The `resample`method takes an argument indicating how the series should be grouped. For example, \"D\" groups the observations into days, \"W\" into weeks, \"M\" into months. \n",
    "\n",
    "One can also group the data into a certain number of days (weeks, months, etc). For example, \"2D\" groups the observations into two-day \"bins\".\n",
    "\n",
    "A complete list of the \"offset aliases\" can be found [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.resample('W').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the length of the time series grouped by the weeks\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(figsize=(16,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(df, test_size=0.1, random_state=7, shuffle=???)\n",
    "\n",
    "train_set.columns = df.columns\n",
    "test_set.columns = df.columns\n",
    "\n",
    "print(f\"{train_set.shape[0]} train and {test_set.shape[0]} test instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Exploratory Data Analysis\n",
    "\n",
    "Let's plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.plot(figsize=(16,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There does not appear any seasonality or trend in the series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Data cleaning and transformation\n",
    "\n",
    "Before we can start building a model, we need to ensure the data is **stationary**. We will use the Augmented Dickey-Fuller (ADF) test and the KPSS (Kwiatkowski-Phillips-Schmidt-Shin) tests to test the series for stationarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "adf_pval = adfuller(train_set['Sales'], maxlag=10, regression='nc')[1]\n",
    "\n",
    "print(\"ADF, p-value:\", adf_pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpss_stat, kpss_pval, lags, crit_vals = kpss(train_set['Sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KPSS, p-value:\", kpss_pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion from these tests???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_diff = train_set['Sales'].diff().dropna()\n",
    "\n",
    "adf_pval = adfuller(train_diff, maxlag=10, regression=\"nc\")[1]\n",
    "print(\"ADF, p-value:\", adf_pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpss_stat, kpss_pval, lags, crit_vals = kpss(train_diff)\n",
    "print(\"KPSS, p-value:\", kpss_pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions from these tests ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_diff = test_set['Sales'].diff().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. Build models\n",
    "\n",
    "## 5.1 Baseline\n",
    "\n",
    "The persistence baseline is generating the previous day's sales as the prediction for this day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_predictions\n",
    "yhat = ???\n",
    "\n",
    "mse = mean_squared_error(test_diff[1:], yhat)\n",
    "\n",
    "baseline_rmse = np.sqrt(mse)\n",
    "baseline_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Extra transformation steps\n",
    "\n",
    "We need to do some transformation steps required to be able to input the data into the scikit-learn's implementation of the ML algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ar_vars(ts, lags=2):\n",
    "    \"\"\"Create autoregressive X variables\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(ts)-lags):\n",
    "        X.append(ts[i:i + lags, 0])\n",
    "        y.append(ts[i + lags, 0])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create separate arrays for the predictors and the target, for both the training and test data. We'll use 3 lags to create autoregressive variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, ytrain = create_ar_vars(train_diff.values.reshape(-1, 1), lags=3)\n",
    "Xtest, ytest = create_ar_vars(test_diff.values.reshape(-1, 1), lags=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both predictor arrays need to be scaled (but the target variable should not be transformed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xtrain = ???\n",
    "Xtest = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use a grid search to find the most optimal hyperparameters settings.\n",
    "\n",
    "## 5.3 Decision Tree regression\n",
    "\n",
    "We'll fine-tune `min_samples_split` (the minimum number of instances required to be at a node before it gets split) and `max_depth` (the maximum depth of each tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeRegressor(random_state=7)\n",
    "param_grid = [\n",
    "    {'max_depth': [None, ???], # try different values and observe the effects on the accuracy\n",
    "    'min_samples_split': [2, ???] # try different values and observe the effects on the accuracy\n",
    "    }\n",
    "]\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "dtree_grid_search = GridSearchCV(estimator=dtree, cv=tscv,\n",
    "                        param_grid=param_grid,\n",
    "                        scoring='neg_mean_squared_error', \n",
    "                        return_train_score=True)\n",
    "\n",
    "start = time.time()\n",
    "dtree_grid_search.fit(Xtrain, ytrain)\n",
    "duration = time.time() - start\n",
    "print(f'Took {duration:.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(dtree_grid_search.cv_results_)[['params', 'mean_train_score', \n",
    "                                                    'mean_test_score']]\n",
    "cv_results[\"mean_train_score\"] = -cv_results[\"mean_train_score\"]\n",
    "cv_results[\"mean_test_score\"] = -cv_results[\"mean_test_score\"]\n",
    "cv_results[\"diff, %\"] = 100*(cv_results[\"mean_train_score\"]-cv_results[\"mean_test_score\"]\n",
    "                                                     )/cv_results[\"mean_train_score\"]\n",
    "\n",
    "cv_results.sort_values('mean_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Random Forest regression\n",
    "\n",
    "We'll fine-tune `n_estimators` (the number of decision trees used in the random forest) as well as `min_samples_split` and `max_depth` (hyperparameters of specific trees)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=7)\n",
    "param_grid = [\n",
    "    {'n_estimators': [10, ???],  # try different values and observe the effects on the accuracy\n",
    "     'max_depth': [None, ???],  # try different values and observe the effects on the accuracy\n",
    "     'min_samples_split': [2, ???]  # try different values and observe the effects on the accuracy\n",
    "    },\n",
    "]\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "rf_grid_search = GridSearchCV(estimator=rf, cv=tscv,\n",
    "                        param_grid=param_grid,\n",
    "                        scoring='neg_mean_squared_error', \n",
    "                        return_train_score=True)\n",
    "\n",
    "start = time.time()\n",
    "rf_grid_search.fit(Xtrain, ytrain)\n",
    "duration = time.time() - start\n",
    "print(f'Took {duration:.3f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the accuracy scores for every model evaluated during the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(rf_grid_search.cv_results_)[['params', 'mean_train_score', \n",
    "                                                    'mean_test_score']]\n",
    "cv_results[\"mean_train_score\"] = -cv_results[\"mean_train_score\"]\n",
    "cv_results[\"mean_test_score\"] = -cv_results[\"mean_test_score\"]\n",
    "cv_results[\"diff, %\"] = 100*(cv_results[\"mean_train_score\"]-cv_results[\"mean_test_score\"]\n",
    "                                                     )/cv_results[\"mean_train_score\"]\n",
    "\n",
    "cv_results.sort_values('mean_test_score', inplace=True)\n",
    "\n",
    "# set the width of the params column\n",
    "cv_results.style.set_properties(subset=['params'], **{'width': '200px'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best models with both DT and RF methods do not seem to overfit too much, and their cross-validation RMSEs are quite above the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. Evaluate the best DT and RF models on the test data\n",
    "\n",
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = ???\n",
    "\n",
    "yhat = best_model.predict(Xtest)\n",
    "\n",
    "dtree_mse = mean_squared_error(ytest, yhat)\n",
    "dtree_rmse = np.sqrt(dtree_mse)\n",
    "dtree_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = ???\n",
    "\n",
    "yhat = best_model.predict(Xtest)\n",
    "\n",
    "rf_mse = mean_squared_error(ytest, yhat)\n",
    "rf_rmse = np.sqrt(rf_mse)\n",
    "rf_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By how much did the decision tree model improve on the persistence baseline, percent-wise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*(baseline_rmse - dtree_rmse)/baseline_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By how much did the RF model improve on the persistence baseline, percent-wise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*(baseline_rmse - rf_rmse)/baseline_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
