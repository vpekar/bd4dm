{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting time series using VAR\n",
    "\n",
    "The task is to build a VAR model to predict the sales amounts in different subcategories of furniture. The data is in \"Superstore.xlsx\", available on Blackboard (the source of the data is [here](https://www.kaggle.com/pruthvi1995/superstore-sales)).\n",
    "\n",
    "The accuracy of the models should be measured in terms of RMSE and compared to a persistence baseline.\n",
    "\n",
    "Please complete the solution by writing code and comments in places indicated with \"???\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting logging to print only error messages from Sklearnex\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"SKLEARNEX\").setLevel(logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Load data\n",
    "\n",
    "We will select only sales relating to Furniture, and will use only the columns \"Order Date\", \"Category\", \"Sub-Category\", and \"Sales\".\n",
    "\n",
    "Note `read_excel` will guess that \"Order Date\" contains dates and will convert the column to the datetime type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Superstore.xlsx\", usecols=[???])\n",
    "df = df[df['Category'] == 'Furniture']\n",
    "\n",
    "# once the relevant rows have been selected, delete the Category column\n",
    "del df[\"Category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the unique subcategories of furniture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sub-Category'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 different subcategories. We will create a dataframe for each, transform each separately, and then merge all of them into one dataframe that can be input into the VAR algorithm.\n",
    "\n",
    "The function `get_sub_category_data` takes the full dataframe, extracts rows corresponding to a given subcategory, calculates daily sales and then groups the daily sales into weekly amounts (in order to smooth out daily fluctuations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_category_data(df, name):\n",
    "    d = df[df['Sub-Category'] == name]\n",
    "    # Obtain daily amounts of sales\n",
    "    d = d.groupby('Order Date').sum()\n",
    "    # Group into weekly amounts\n",
    "    d = d.resample('W').sum()\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the dataframe produced for the \"Chairs\" subcategory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chairs_df = get_sub_category_data(df, \"Chairs\")\n",
    "chairs_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(chairs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chairs_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go over the subcategories, produce a dataframe for each, storing them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcat_dfs = []\n",
    "for x in df['Sub-Category'].unique():\n",
    "    d = get_sub_category_data(df, x)\n",
    "    d.columns = [x]\n",
    "    print(f\"Produced a df for {x}, its shape is {d.shape}\")\n",
    "    subcat_dfs.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the four dataframes have around 208 rows, which is the number of rows in the original dataset, if it is grouped into weekly amounts of sales. There is, however, 3 weeks with no sales for Tables and 1 week with no sales for Bookcases.\n",
    "\n",
    "Now, merge them one by one. We will use the intersection, that is, throwing away those rows for which at least one other subcategory has no sales data. So we are effectively deleting rows with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the first dataframe into df2, the output dataframe\n",
    "\n",
    "df2 = subcat_dfs[0]\n",
    "for x in subcat_dfs[1:]:\n",
    "    # indicate that we merge on the indices of the two dataframes\n",
    "    df2 = pd.merge(df2, x, how=\"inner\", left_index=True, right_index=True)\n",
    "\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the final dataset looks like: we have weekly observations, across the four subcategories of furniture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Train-test split\n",
    "\n",
    "The produced dataframe can now be split into the train and test parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "???\n",
    "\n",
    "# make sure the training and test sets have the same column name as dfs\n",
    "train_set.columns = df2.columns\n",
    "test_set.columns = df2.columns\n",
    "\n",
    "print(f\"{train_set.shape[0]} train and {test_set.shape[0]} test instances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Exploratory Data Analysis\n",
    "\n",
    "Let's plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.plot(figsize=(16,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is quite a bit of volatility within specific subcategories, and there is no apparent relationship between the four time series, so the forecasting problem may be quite hard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Data cleaning and transformation\n",
    "\n",
    "Before we can start building a model, we need to ensure the data is **stationary**. We will use the Augmented Dickey-Fuller (ADF) test and the KPSS (Kwiatkowski-Phillips-Schmidt-Shin) tests to test the series for stationarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "\n",
    "# test each time series one by one\n",
    "for x in train_set.columns:\n",
    "    print(x)\n",
    "    ???\n",
    "    print(f\"ADF, p-value: {adf_pval}\")\n",
    "    ???\n",
    "    print(f\"KPSS, p-value: {kpss_pval}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment??? (2-3 sentences)\n",
    "\n",
    "For consistency, we will difference all the four series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "???\n",
    "train_diff.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_diff.columns:\n",
    "    print(x)\n",
    "    ???\n",
    "    print(f\"ADF, p-value: {adf_pval:.3f}\")\n",
    "    ???\n",
    "    print(f\"KPSS, p-value: {kpss_pval:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment??? (2-3 sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "???\n",
    "test_diff.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5. Build models\n",
    "\n",
    "## 5.1 Baseline\n",
    "\n",
    "The persistence baseline is generating the previous day's sales as the prediction for this day.\n",
    "\n",
    "We'll generate a baseline for each subcategory separately and then calculate their average RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_rmse = {}\n",
    "for x in test_diff.columns:\n",
    "    baseline_predictions = test_diff[x].shift()[1:]\n",
    "    mse = ???\n",
    "    baseline_rmse[x] = np.sqrt(mse)\n",
    "    print(f\"{x}: {baseline_rmse[x]:.3f}\")\n",
    "\n",
    "aver = np.array(list(baseline_rmse.values())).mean()\n",
    "\n",
    "print(f\"\\nAverage over the subcategories: {aver:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 VAR models\n",
    "\n",
    "The first step is to select the order of VAR, i.e. the optimal number of lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "\n",
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "??? (Comment on the choice of the number of lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_model = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "??? (2-3 sentences commenting on the constructed model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. Evaluate the model on the test set\n",
    "\n",
    "We'll use the same evaluation setup as with the ARIMA model: we'll make one-step-ahead forecasts, and we will re-train the model on training data plus any test data that has already been used for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffers keeping training + previously seen test data\n",
    "history = train_diff.values\n",
    "predictions = []\n",
    "\n",
    "# for each test observation, take the first 200 for convenience\n",
    "for i, test_obs in enumerate(test_diff.values):\n",
    "    \n",
    "    # build a model using the current buffers\n",
    "    model = VAR(history).fit(2)\n",
    "        \n",
    "    # forecast the value for the test instance, supplying corresponding exogenous variables \n",
    "    yhat = model.forecast(model.y, steps=1)\n",
    "    \n",
    "    # remember the forecasted value\n",
    "    predictions.append(yhat[0])\n",
    "    \n",
    "    # update the buffers for the endogenous and exogenous variables\n",
    "    history = np.append(history, test_obs.reshape(1, -1), axis=0)\n",
    "\n",
    "predictions = pd.DataFrame(predictions, columns=test_diff.columns, index=test_diff.index)\n",
    "\n",
    "predictions.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's calculate the RMSE scores within each subcategory and the average RMSE across the subcategories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_rmse = {}\n",
    "for x in test_diff.columns:\n",
    "    mse = ???\n",
    "    var_rmse[x] = np.sqrt(mse)\n",
    "    print(f\"{x} RMSE: {var_rmse[x]:.3f}\")\n",
    "\n",
    "aver = np.array(list(var_rmse.values())).mean()\n",
    "print(f\"Average over subcategories: {aver:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare these results to those achieved by the baseline method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame([baseline_rmse, var_rmse]).transpose()\n",
    "scores_df.columns = [\"Baseline\", \"VAR\"]\n",
    "\n",
    "# add a column showing error rate reduction compared to the baseline\n",
    "scores_df[\"% reduction\"] = 100*(scores_df[\"Baseline\"]-scores_df[\"VAR\"])/scores_df[\"Baseline\"]\n",
    "\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df[\"% reduction\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "??? (one-two sentences on the quality of the constructed VAR model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
